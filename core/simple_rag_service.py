#!/usr/bin/env python3
"""
ç®€åŒ–çš„RAGæœåŠ¡æ¨¡å— - ç²¾ç®€ç‰ˆ
æ•´åˆå‘é‡å­˜å‚¨å’Œæ–‡æ¡£å¤„ç†åŠŸèƒ½
"""

from typing import List, Generator
from pathlib import Path
from datetime import datetime

try:
    from langchain_community.embeddings import OllamaEmbeddings
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    from langchain.schema import Document
    from langchain_community.document_loaders import PyPDFLoader, TextLoader
    DEPENDENCIES_AVAILABLE = True
except ImportError as e:
    print(f"âŒ RAGæœåŠ¡ä¾èµ–ä¸å¯ç”¨: {e}")
    DEPENDENCIES_AVAILABLE = False

from .config import config
from .models import ChatModel
from vector_stores.memory_vector_store import MemoryVectorStore
from vector_stores.faiss_vector_store import FAISSVectorStore
from vector_stores.vector_config import get_store_config, list_available_stores


class SimpleRAGService:
    """ç®€åŒ–çš„RAGæœåŠ¡"""
    
    def __init__(self, vector_store_type: str = "auto", store_path: str = None):
        if not DEPENDENCIES_AVAILABLE:    # ä¾èµ–åŒ…ä¸å¯ç”¨
            raise ImportError("RAGæœåŠ¡ä¾èµ–ä¸å¯ç”¨")
        
        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        self.embeddings = OllamaEmbeddings(
            base_url=config.ollama_base_url,
            model=config.ollama_embedding_model
        )
        
        # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„è·¯å¾„ï¼Œå¦‚æœæœªæŒ‡å®šçš„è¯
        self.store_path = store_path or config.get_vector_db_path()
        
        # åˆå§‹åŒ–å‘é‡å­˜å‚¨
        self.vector_store = self._create_vector_store(vector_store_type, self.store_path)
        self.vector_store_type = vector_store_type
        
        # æ–‡æ¡£è·Ÿè¸ª - ä½¿ç”¨é…ç½®ä¸­çš„å…ƒæ•°æ®è·¯å¾„
        # ç¡®ä¿dataç›®å½•å­˜åœ¨
        data_dir = Path(config.get_vector_db_path()).parent
        data_dir.mkdir(exist_ok=True)
        
        self.metadata_file_path = Path(config.get_document_metadata_path())
        self.document_metadata = self._load_document_metadata()  # å°è¯•åŠ è½½å·²ä¿å­˜çš„metadata
        self.next_doc_id = self._get_next_doc_id()  # åŸºäºç°æœ‰metadataç¡®å®šä¸‹ä¸€ä¸ªID
        
        # åˆå§‹åŒ–èŠå¤©æ¨¡å‹
        self.chat_model = ChatModel()
        
        # æ–‡æœ¬åˆ†å‰²å™¨
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )
        
        print(f"SUCCESS: RAGæœåŠ¡åˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨: {self._get_current_store_name()}")
    
    def _load_document_metadata(self):
        """åŠ è½½æ–‡æ¡£metadata"""
        try:
            import json
            if Path(self.metadata_file_path).exists():
                with open(self.metadata_file_path, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                    print(f"ğŸ“‚ å·²åŠ è½½ {len(metadata)} ä¸ªæ–‡æ¡£çš„metadata")
                    return metadata
        except Exception as e:
            print(f"âš ï¸  åŠ è½½metadataæ—¶å‡ºé”™: {e}")
        return {}
    
    def _save_document_metadata(self):
        """ä¿å­˜æ–‡æ¡£metadata"""
        try:
            import json
            with open(self.metadata_file_path, 'w', encoding='utf-8') as f:
                json.dump(self.document_metadata, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âš ï¸  ä¿å­˜metadataæ—¶å‡ºé”™: {e}")
    
    def _get_next_doc_id(self):
        """åŸºäºç°æœ‰metadataç¡®å®šä¸‹ä¸€ä¸ªæ–‡æ¡£ID"""
        if not self.document_metadata:
            return 0
        # æ‰¾åˆ°æœ€å¤§çš„æ•°å­—IDå¹¶åŠ 1
        max_id = -1
        for doc_id in self.document_metadata.keys():
            try:
                num_id = int(doc_id)
                max_id = max(max_id, num_id)
            except ValueError:
                pass  # å¿½ç•¥éæ•°å­—ID
        return max_id + 1
    
    def _create_vector_store(self, store_type: str, store_path: str):
        """åˆ›å»ºå‘é‡å­˜å‚¨"""
        if store_type == "auto":
            # è‡ªåŠ¨é€‰æ‹©å¯ç”¨çš„å‘é‡å­˜å‚¨
            available_stores = list_available_stores()
            config = get_store_config("auto")
            
            for preferred_type in config["priority"]:
                if preferred_type in available_stores:
                    store_type = preferred_type
                    break
            else:
                raise RuntimeError("æ²¡æœ‰å¯ç”¨çš„å‘é‡å­˜å‚¨ç±»å‹")
        
        # åˆ›å»ºå…·ä½“çš„å‘é‡å­˜å‚¨å®ä¾‹
        if store_type == "memory":
            return MemoryVectorStore(self.embeddings)
        elif store_type.startswith("faiss_"):
            store_config = get_store_config(store_type)
            index_type = store_config.get("index_type", "IndexFlatL2")
            vector_store = FAISSVectorStore(self.embeddings, index_type, store_path)
            # å°è¯•åŠ è½½ç°æœ‰å­˜å‚¨
            vector_store.load()
            return vector_store
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„å‘é‡å­˜å‚¨ç±»å‹: {store_type}")
    
    def _get_current_store_name(self) -> str:
        """è·å–å½“å‰å­˜å‚¨ç±»å‹çš„æ˜¾ç¤ºåç§°"""
        info = self.vector_store.get_info()
        return info.get("type", "unknown")
    
    def process_document(self, file_path: str, file_content: bytes = None) -> tuple[bool, str]:
        """å¤„ç†æ–‡æ¡£å¹¶æ·»åŠ åˆ°å‘é‡å­˜å‚¨ï¼Œè¿”å›(æˆåŠŸçŠ¶æ€, æ–‡æ¡£ID)"""
        try:
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not Path(file_path).exists():
                return False, None
            
            filename = Path(file_path).name
            
            # åŠ è½½æ–‡æ¡£
            file_extension = Path(file_path).suffix.lower() #åç¼€
            
            if file_extension == '.pdf':
                loader = PyPDFLoader(file_path)
                documents = loader.load()   # åŠ è½½ï¼ˆæ¯é¡µä¸€ä¸ª Documentï¼‰
            elif file_extension == '.txt':
                # ç›´æ¥è¯»å–æ–‡æœ¬æ–‡ä»¶å†…å®¹ï¼Œæ”¯æŒå¤šç§ç¼–ç 
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                except UnicodeDecodeError:
                    try:
                        with open(file_path, 'r', encoding='gbk') as f:
                            content = f.read()
                    except UnicodeDecodeError:
                        with open(file_path, 'r', encoding='latin1') as f:
                            content = f.read()
                
                # åˆ›å»ºDocumentå¯¹è±¡
                from langchain.schema import Document
                documents = [Document(page_content=content, metadata={"source": file_path})]
            else:
                return False, None

            # åˆ†å‰²æ–‡æ¡£
            chunks = self.text_splitter.split_documents(documents)
            
            # ä¸ºæ¯ä¸ªchunkæ·»åŠ document_idåˆ°metadata
            doc_id = str(self.next_doc_id)
            for chunk in chunks:
                chunk.metadata["document_id"] = doc_id
                chunk.metadata["filename"] = filename
            
            # æ·»åŠ åˆ°å‘é‡å­˜å‚¨
            success = self.vector_store.add_documents(chunks)
            
            if success:
                # è®°å½•æ–‡æ¡£ä¿¡æ¯
                self.document_metadata[doc_id] = {
                    "filename": filename,
                    "chunks": len(chunks),
                    "timestamp": datetime.now().isoformat(),
                    "file_path": file_path,
                    "deleted": False  # è½¯åˆ é™¤æ ‡è®°ä½
                }
                self.next_doc_id += 1
                
                if hasattr(self.vector_store, 'save'):
                    save_result = self.vector_store.save()

                
                # ä¿å­˜document metadata
                self._save_document_metadata()
                
                return True, doc_id
            else:
                print(f"âŒ å‘é‡å­˜å‚¨æ·»åŠ å¤±è´¥")
            
            return False, None
            
        except Exception as e:
            print(f"âŒ å¤„ç†æ–‡æ¡£æ—¶å‡ºç°å¼‚å¸¸: {e}")
            import traceback
            traceback.print_exc()
            return False, None
    
    def rag_chat(self, query: str, use_context: bool = True) -> str:
        """RAGèŠå¤©"""
        if not use_context:
            return self.chat_model.generate_response(query)
        
        try:
            # æ£€ç´¢ç›¸å…³æ–‡æ¡£ - è¿‡æ»¤å·²åˆ é™¤çš„æ–‡æ¡£
            search_k = min(9, 20)  # æœç´¢æ›´å¤šç»“æœä»¥åº”å¯¹åˆ é™¤è¿‡æ»¤
            results = self.vector_store.similarity_search_with_score(query, k=search_k) #è¿‡æ»¤å·²åˆ é™¤çš„æ–‡æ¡£
            filtered_results = self._filter_deleted_documents(results)
            filtered_results = filtered_results[:3]  # é™åˆ¶æœ€ç»ˆç»“æœä¸º3ä¸ª
            
            if not filtered_results:
                return self.chat_model.generate_response(query, "æ³¨æ„ï¼šæ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–‡æ¡£ï¼Œè¯·åŸºäºå¸¸è¯†å›ç­”ã€‚")
            
            # æ„å»ºä¸Šä¸‹æ–‡
            context_parts = []
            for doc, score in filtered_results:
                context_parts.append(f"æ–‡æ¡£å†…å®¹ï¼š{doc.page_content}")
            
            context = "\n\n".join(context_parts)
            
            return self.chat_model.generate_response(query, context)
            
        except Exception as e:
            return self.chat_model.generate_response(query)
    
    def rag_chat_stream(self, query: str, use_context: bool = True) -> Generator[str, None, None]:
        """RAGæµå¼èŠå¤©"""
        if not use_context:
            yield from self.chat_model.generate_stream_response(query)
            return
        
        try:
            # æ£€ç´¢ç›¸å…³æ–‡æ¡£ - è¿‡æ»¤å·²åˆ é™¤çš„æ–‡æ¡£
            search_k = min(9, 20)  # æœç´¢æ›´å¤šç»“æœä»¥åº”å¯¹åˆ é™¤è¿‡æ»¤
            results = self.vector_store.similarity_search_with_score(query, k=search_k)
            filtered_results = self._filter_deleted_documents(results)
            filtered_results = filtered_results[:3]  # é™åˆ¶æœ€ç»ˆç»“æœä¸º3ä¸ª
            
            if not filtered_results:
                yield from self.chat_model.generate_stream_response(query, "æ³¨æ„ï¼šæ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–‡æ¡£ï¼Œè¯·åŸºäºå¸¸è¯†å›ç­”ã€‚")
            else:
                # æ„å»ºä¸Šä¸‹æ–‡
                context_parts = []
                for doc, score in filtered_results:
                    context_parts.append(f"æ–‡æ¡£å†…å®¹ï¼š{doc.page_content}")
                
                context = "\n\n".join(context_parts)
                yield from self.chat_model.generate_stream_response(query, context)
            
        except Exception as e:
            yield from self.chat_model.generate_stream_response(query)
    
    def get_status(self) -> dict:
        """è·å–RAGæœåŠ¡çŠ¶æ€"""
        store_info = self.vector_store.get_info()
        
        # æ·»åŠ æ–‡æ¡£è®¡æ•°ä¿¡æ¯
        store_info["documents"] = len(self.document_metadata)
        store_info["document_list"] = list(self.document_metadata.keys())
        
        return {
            "available": True,
            "vector_store": store_info,
            "embedding_model": config.ollama_embedding_model,
            "chat_model": config.ollama_model
        }
    
    def _filter_deleted_documents(self, search_results):
        """è¿‡æ»¤å·²åˆ é™¤çš„æ–‡æ¡£"""
        filtered_results = []
        for doc, score in search_results:
            # ç›´æ¥ä»metadataä¸­è·å–document_id
            document_id = doc.metadata.get('document_id')
            
            # å¦‚æœæ–‡æ¡£æœ‰document_idä¸”è¢«æ ‡è®°ä¸ºåˆ é™¤ï¼Œåˆ™è·³è¿‡
            if document_id and self.document_metadata.get(document_id, {}).get('deleted', False):
                continue  # è·³è¿‡å·²åˆ é™¤çš„æ–‡æ¡£
                
            filtered_results.append((doc, score))
        
        return filtered_results

    def search_documents(self, query: str, k: int = 3) -> list:
        """æœç´¢ç›¸å…³æ–‡æ¡£ - è‡ªåŠ¨è¿‡æ»¤å·²åˆ é™¤çš„æ–‡æ¡£"""
        try:
            # è·å–æ›´å¤šç»“æœä»¥åº”å¯¹åˆ é™¤è¿‡æ»¤
            search_k = min(k * 3, 20)  # æœç´¢æ›´å¤šç»“æœï¼Œä½†é™åˆ¶åœ¨åˆç†èŒƒå›´å†…
            results = self.vector_store.similarity_search_with_score(query, k=search_k)
            
            # è¿‡æ»¤å·²åˆ é™¤çš„æ–‡æ¡£
            filtered_results = self._filter_deleted_documents(results)
            
            # é™åˆ¶è¿”å›ç»“æœæ•°é‡
            filtered_results = filtered_results[:k]
            
            documents = []
            for doc, score in filtered_results:
                documents.append({
                    "content": doc.page_content,
                    "metadata": doc.metadata,
                    "score": float(score)
                })
            return documents
        except Exception as e:
            return []
    
    def delete_document(self, document_id: str) -> dict:
        """è½¯åˆ é™¤æ–‡æ¡£ - é€šè¿‡æ ‡è®°ä½å®ç°"""
        try:
            # æ£€æŸ¥æ–‡æ¡£æ˜¯å¦å­˜åœ¨
            if document_id not in self.document_metadata:
                available_ids = list(self.document_metadata.keys())
                return {
                    "success": False,
                    "message": f"æ–‡æ¡£ID {document_id} ä¸å­˜åœ¨",
                    "error": f"å½“å‰å¯ç”¨çš„æ–‡æ¡£ID: {available_ids}ã€‚æ€»å…±æœ‰ {len(available_ids)} ä¸ªæ–‡æ¡£ã€‚"
                }
            
            doc_info = self.document_metadata[document_id]
            
            # æ£€æŸ¥æ˜¯å¦å·²ç»è¢«åˆ é™¤
            if doc_info.get("deleted", False):
                return {
                    "success": False,
                    "message": f"æ–‡æ¡£ '{doc_info['filename']}' å·²ç»è¢«åˆ é™¤",
                    "error": "æ­¤æ–‡æ¡£å·²è¢«æ ‡è®°ä¸ºåˆ é™¤çŠ¶æ€"
                }
            
            # è½¯åˆ é™¤ï¼šæ ‡è®°ä¸ºåˆ é™¤çŠ¶æ€
            self.document_metadata[document_id]["deleted"] = True
            self.document_metadata[document_id]["deleted_timestamp"] = datetime.now().isoformat()
            
            # ä¿å­˜æ›´æ–°çš„metadata
            self._save_document_metadata()
            
            return {
                "success": True,
                "message": f"æ–‡æ¡£ '{doc_info['filename']}' å·²æˆåŠŸåˆ é™¤",
                "detail": "æ–‡æ¡£å·²è¢«æ ‡è®°ä¸ºåˆ é™¤ï¼Œä¸ä¼šåœ¨æœç´¢ä¸­å‡ºç°"
            }
                
        except Exception as e:
            return {
                "success": False,
                "message": "åˆ é™¤æ–‡æ¡£æ—¶å‘ç”Ÿé”™è¯¯",
                "error": f"æŠ€æœ¯è¯¦æƒ…: {str(e)}"
            }
    
    def list_documents(self, include_deleted: bool = False) -> list:
        """åˆ—å‡ºæ–‡æ¡£ï¼ˆé»˜è®¤ä¸åŒ…å«å·²åˆ é™¤çš„æ–‡æ¡£ï¼‰"""
        try:
            documents = []
            for doc_id, doc_info in self.document_metadata.items():
                # å¦‚æœä¸åŒ…å«å·²åˆ é™¤æ–‡æ¡£ï¼Œåˆ™è·³è¿‡å·²åˆ é™¤çš„
                if not include_deleted and doc_info.get("deleted", False):
                    continue
                    
                documents.append({
                    "id": doc_id,
                    "name": doc_info["filename"],
                    "chunks": doc_info["chunks"],
                    "timestamp": doc_info["timestamp"],
                    "file_path": doc_info.get("file_path", ""),
                    "deleted": doc_info.get("deleted", False)
                })
            return documents
        except Exception as e:
            return []
    
    def clear_store(self) -> dict:
        """æ¸…ç©ºå‘é‡å­˜å‚¨"""
        try:
            # é‡æ–°åˆ›å»ºç©ºçš„å‘é‡å­˜å‚¨
            self._create_vector_store(self.store_type, self.store_path)
            return {
                "success": True,
                "message": "å‘é‡å­˜å‚¨å·²æ¸…ç©º"
            }
        except Exception as e:
            return {
                "success": False,
                "message": "æ¸…ç©ºå­˜å‚¨æ—¶å‡ºé”™",
                "error": str(e)
            }